[
  {
    "objectID": "HW5_code.html",
    "href": "HW5_code.html",
    "title": "ST558: Homework 5",
    "section": "",
    "text": "Loading required package: tidyverse\n\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tibble' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: caret\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nLoading required package: rpart\nLoading required package: randomForest\n\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nLoading required package: gbm\n\n\nWarning: package 'gbm' was built under R version 4.3.3\n\n\nLoaded gbm 2.2.2\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\n\nTask 1: Conceptual Questions\n\nWhat is the purpose of using cross-validation when fitting a random forest model?\n\nCross Validation helps see how well a random forest model is working on different parts of the data. This prevents overfitting and provides a more accurate estimate.\n\nDescribe the bagged tree algorithm.\n\nBagging (Bootstrap Aggregating) trains several decision trees on bootstrapped data samples and combines their results by regression or classification to provide better results.\n\nWhat is meant by a general linear model?\n\nA GLM is a linear model that is extended to accomadate for response variables with different distributions and uses a link function to relate the predictors to the response variable.\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nAdding an interaction term allows the model to capture a combined effect of two predictors on the response and displays their relationship changes between one predictor with different values of another.\n\nWhy do we split our data into a training and test set?\n\nSplitting the data allows us to train the model on one subset and validate its performance on a new one. This ensures that it can make accurate predictions on the new data based on what it has learned from the training set.\n\n\nTask 2: Fitting Models\nFor this Homework assignment we will be using the “hearts.csv” dataset which indicate whether or not someone has a heart disease by HeartDisease = 1 or = 0.\n\n# Load dataset\nheart_dat &lt;- read.csv(\"heart.csv\")\n\n# Check for missing values\nmissing &lt;- colSums(is.na(heart_dat))\nprint(missing)\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n# Summarize dataset\nsummary(heart_dat)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n# Convert HeartDisease to a factor\nheart_dat$HeartDisease &lt;- as.factor(heart_dat$HeartDisease)\n\n# Remove ST_Slope column\nheart_dat &lt;- heart_dat %&gt;% select(-ST_Slope)\n\n# Create dummy variables for categorical variables\ncategorical_vars &lt;- c(\"Sex\", \"ExerciseAngina\", \"ChestPainType\", \"RestingECG\")\ndummy_vars &lt;- dummyVars(~ ., data = heart_dat[categorical_vars])\ndummy_data &lt;- predict(dummy_vars, heart_dat[categorical_vars])\n\n# Combine dummy variables with the original dataset\nheart_dat &lt;- heart_dat %&gt;%\n  select(-one_of(categorical_vars)) %&gt;%\n  bind_cols(as.data.frame(dummy_data))\n\nKNN with Split Dataset\n\n# Split the data into training and test sets\nset.seed(123)\ntrain_index &lt;- createDataPartition(heart_dat$HeartDisease, p = 0.7, list = FALSE)\ntrain_data &lt;- heart_dat[train_index, ]\ntest_data &lt;- heart_dat[-train_index, ]\n\n# Set up 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n# Set up the grid for k\ntune_grid &lt;- expand.grid(k = 1:40)\n\n# Train the kNN model\nknn_model &lt;- train(\n  HeartDisease ~ ., \n  data = train_data,\n  method = \"knn\",\n  trControl = train_control,\n  preProcess = c(\"center\", \"scale\"),\n  tuneGrid = tune_grid\n)\n\nknn_model\n\nk-Nearest Neighbors \n\n643 samples\n 17 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 580, 578, 578, 579, 578, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7578485  0.5115282\n   2  0.7377040  0.4733859\n   3  0.7780744  0.5527047\n   4  0.7759828  0.5489545\n   5  0.7863441  0.5704546\n   6  0.7904387  0.5779413\n   7  0.7998723  0.5964296\n   8  0.8019719  0.6002275\n   9  0.8013785  0.5991926\n  10  0.7962177  0.5882800\n  11  0.8014348  0.5988616\n  12  0.7977882  0.5914950\n  13  0.8029653  0.6021097\n  14  0.7952163  0.5875912\n  15  0.8009135  0.5992020\n  16  0.7992621  0.5955891\n  17  0.7993344  0.5962211\n  18  0.7971945  0.5911453\n  19  0.7992939  0.5957385\n  20  0.8019146  0.6006240\n  21  0.8008561  0.5987639\n  22  0.7998064  0.5966240\n  23  0.8039576  0.6045527\n  24  0.7967135  0.5896521\n  25  0.7998224  0.5957062\n  26  0.7988208  0.5933232\n  27  0.7946785  0.5844760\n  28  0.7941571  0.5837171\n  29  0.7952316  0.5854417\n  30  0.7993582  0.5942385\n  31  0.7982842  0.5915807\n  32  0.7982840  0.5912801\n  33  0.7982762  0.5910646\n  34  0.8008726  0.5965976\n  35  0.8003841  0.5947967\n  36  0.8014012  0.5969293\n  37  0.8004001  0.5947639\n  38  0.7993259  0.5925327\n  39  0.8035337  0.6007435\n  40  0.8003756  0.5940410\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 23.\n\n# Plot the results of the kNN model\nplot(knn_model)\n\n\n\n# Predict on the test set\npredictions_knn &lt;- predict(knn_model, newdata = test_data)\n\n# Evaluate the kNN model using confusion matrix\nconf_matrix_knn &lt;- confusionMatrix(predictions_knn, test_data$HeartDisease)\nprint(conf_matrix_knn)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 109  27\n         1  14 125\n                                          \n               Accuracy : 0.8509          \n                 95% CI : (0.8032, 0.8908)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.7015          \n                                          \n Mcnemar's Test P-Value : 0.06092         \n                                          \n            Sensitivity : 0.8862          \n            Specificity : 0.8224          \n         Pos Pred Value : 0.8015          \n         Neg Pred Value : 0.8993          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3964          \n   Detection Prevalence : 0.4945          \n      Balanced Accuracy : 0.8543          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nLogistic Regression Models\n\n# Model 1: Using all predictors\nmodel_1 &lt;- train(\n  HeartDisease ~ ., \n  data = train_data,\n  method = \"glm\",\n  family = binomial,\n  trControl = train_control,\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Model 2: Using a subset of predictors\nmodel_2 &lt;- train(\n  HeartDisease ~ Age + Cholesterol + MaxHR + SexM + ChestPainTypeATA,\n  data = train_data,\n  method = \"glm\",\n  family = binomial,\n  trControl = train_control,\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Model 3: Using a different subset\nmodel_3 &lt;- train(\n  HeartDisease ~ Age + RestingBP + FastingBS + ExerciseAnginaY + RestingECGLVH,\n  data = train_data,\n  method = \"glm\",\n  family = binomial,\n  trControl = train_control,\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Print summaries of the logistic regression models\nsummary(model_1)\n\n\nCall:\nNULL\n\nCoefficients: (4 not defined because of singularities)\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.37248    0.11450   3.253 0.001141 ** \nAge               0.12258    0.12891   0.951 0.341647    \nRestingBP         0.03523    0.11335   0.311 0.755922    \nCholesterol      -0.37944    0.12721  -2.983 0.002856 ** \nFastingBS         0.45760    0.12116   3.777 0.000159 ***\nMaxHR            -0.39647    0.13366  -2.966 0.003014 ** \nOldpeak           0.56285    0.13097   4.298 1.73e-05 ***\nSexF             -0.45630    0.11498  -3.969 7.23e-05 ***\nSexM                   NA         NA      NA       NA    \nExerciseAnginaN  -0.49229    0.12446  -3.956 7.63e-05 ***\nExerciseAnginaY        NA         NA      NA       NA    \nChestPainTypeASY  0.76818    0.24744   3.104 0.001906 ** \nChestPainTypeATA -0.11378    0.21110  -0.539 0.589903    \nChestPainTypeNAP -0.01392    0.21535  -0.065 0.948446    \nChestPainTypeTA        NA         NA      NA       NA    \nRestingECGLVH     0.15098    0.15133   0.998 0.318429    \nRestingECGNormal -0.02081    0.14917  -0.139 0.889075    \nRestingECGST           NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 524.98  on 629  degrees of freedom\nAIC: 552.98\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(model_2)\n\n\nCall:\nNULL\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.21863    0.09899   2.209   0.0272 *  \nAge               0.23934    0.10418   2.297   0.0216 *  \nCholesterol      -0.26049    0.10284  -2.533   0.0113 *  \nMaxHR            -0.69437    0.11534  -6.020 1.74e-09 ***\nSexM              0.51431    0.10033   5.126 2.96e-07 ***\nChestPainTypeATA -0.70212    0.11659  -6.022 1.72e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 656.37  on 637  degrees of freedom\nAIC: 668.37\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(model_3)\n\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      0.33583    0.09609   3.495 0.000474 ***\nAge              0.37037    0.10240   3.617 0.000298 ***\nRestingBP       -0.01370    0.09722  -0.141 0.887948    \nFastingBS        0.55773    0.10197   5.469 4.52e-08 ***\nExerciseAnginaY  1.03301    0.10013  10.317  &lt; 2e-16 ***\nRestingECGLVH    0.03107    0.09493   0.327 0.743483    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 683.49  on 637  degrees of freedom\nAIC: 695.49\n\nNumber of Fisher Scoring iterations: 4\n\n# Compare the logistic regression models based on cross-validated performance\nresamples_list &lt;- resamples(list(model_1 = model_1, model_2 = model_2, model_3 = model_3))\nsummary(resamples_list)\n\n\nCall:\nsummary.resamples(object = resamples_list)\n\nModels: model_1, model_2, model_3 \nNumber of resamples: 30 \n\nAccuracy \n             Min.   1st Qu.    Median      Mean   3rd Qu.    Max. NA's\nmodel_1 0.6718750 0.7656250 0.7984375 0.7926530 0.8281250 0.87500    0\nmodel_2 0.6875000 0.7500000 0.7597356 0.7594759 0.7846154 0.84375    0\nmodel_3 0.6507937 0.7042668 0.7519231 0.7464096 0.7782452 0.84375    0\n\nKappa \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel_1 0.3359684 0.5287184 0.5914390 0.5807361 0.6563739 0.7419355    0\nmodel_2 0.3495935 0.4819380 0.5095092 0.5057618 0.5567307 0.6800000    0\nmodel_3 0.2877698 0.4180571 0.4938563 0.4888409 0.5526049 0.6884129    0\n\ndotplot(resamples_list)\n\n\n\n# Choose the best logistic regression model\nbest_model_logistic &lt;- model_1\n\n# Predictions on the test set using the best logistic regression model\npredictions_logistic &lt;- predict(best_model_logistic, newdata = test_data)\n\n# Logistic regression model\nconf_matrix_logistic &lt;- confusionMatrix(predictions_logistic, test_data$HeartDisease)\nprint(conf_matrix_logistic)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 106  21\n         1  17 131\n                                          \n               Accuracy : 0.8618          \n                 95% CI : (0.8153, 0.9003)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.7214          \n                                          \n Mcnemar's Test P-Value : 0.6265          \n                                          \n            Sensitivity : 0.8618          \n            Specificity : 0.8618          \n         Pos Pred Value : 0.8346          \n         Neg Pred Value : 0.8851          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3855          \n   Detection Prevalence : 0.4618          \n      Balanced Accuracy : 0.8618          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nTree Models\n\n### Classification Tree Model\n# Set up the grid for tuning the parameter cp\ntune_grid_rpart &lt;- expand.grid(cp = seq(0, 0.1, by = 0.001))\n\n# Train the classification tree model\nrpart_model &lt;- train(\n  HeartDisease ~ ., \n  data = train_data,\n  method = \"rpart\",\n  trControl = train_control,\n  tuneGrid = tune_grid_rpart\n)\n\n# Print the results of the classification tree model\nprint(rpart_model)\n\nCART \n\n643 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 578, 579, 578, 580, 578, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7550694  0.5032057\n  0.001  0.7555903  0.5043038\n  0.002  0.7581789  0.5083975\n  0.003  0.7581789  0.5083975\n  0.004  0.7545484  0.5012092\n  0.005  0.7524645  0.4966936\n  0.006  0.7566562  0.5062506\n  0.007  0.7582270  0.5097459\n  0.008  0.7593087  0.5120219\n  0.009  0.7593087  0.5120219\n  0.010  0.7603667  0.5136268\n  0.011  0.7578026  0.5071907\n  0.012  0.7635565  0.5189861\n  0.013  0.7640774  0.5191314\n  0.014  0.7656164  0.5231639\n  0.015  0.7687414  0.5285835\n  0.016  0.7739016  0.5372492\n  0.017  0.7749598  0.5386228\n  0.018  0.7749598  0.5386228\n  0.019  0.7765471  0.5414804\n  0.020  0.7733971  0.5355350\n  0.021  0.7733971  0.5355350\n  0.022  0.7708169  0.5295690\n  0.023  0.7708169  0.5295690\n  0.024  0.7697913  0.5280323\n  0.025  0.7697913  0.5280323\n  0.026  0.7682203  0.5243194\n  0.027  0.7682203  0.5243194\n  0.028  0.7656241  0.5187076\n  0.029  0.7656241  0.5187076\n  0.030  0.7656241  0.5187076\n  0.031  0.7671866  0.5213204\n  0.032  0.7676994  0.5220994\n  0.033  0.7666412  0.5202990\n  0.034  0.7666412  0.5202990\n  0.035  0.7640776  0.5159907\n  0.036  0.7640776  0.5159907\n  0.037  0.7640776  0.5163181\n  0.038  0.7640776  0.5163181\n  0.039  0.7604233  0.5100013\n  0.040  0.7604233  0.5100013\n  0.041  0.7604233  0.5100013\n  0.042  0.7604233  0.5100013\n  0.043  0.7599105  0.5092827\n  0.044  0.7599105  0.5092827\n  0.045  0.7599105  0.5092827\n  0.046  0.7599105  0.5092827\n  0.047  0.7599105  0.5107438\n  0.048  0.7599105  0.5107438\n  0.049  0.7599105  0.5107438\n  0.050  0.7599105  0.5107438\n  0.051  0.7604313  0.5122028\n  0.052  0.7604313  0.5122028\n  0.053  0.7604313  0.5122028\n  0.054  0.7604313  0.5122028\n  0.055  0.7609361  0.5142541\n  0.056  0.7609361  0.5142541\n  0.057  0.7609361  0.5142541\n  0.058  0.7598779  0.5122976\n  0.059  0.7609035  0.5155510\n  0.060  0.7609035  0.5155510\n  0.061  0.7609035  0.5155510\n  0.062  0.7603744  0.5146603\n  0.063  0.7593408  0.5128807\n  0.064  0.7593408  0.5128807\n  0.065  0.7593408  0.5128807\n  0.066  0.7619863  0.5191976\n  0.067  0.7619863  0.5191976\n  0.068  0.7619863  0.5191976\n  0.069  0.7619863  0.5191976\n  0.070  0.7619863  0.5191976\n  0.071  0.7619863  0.5191976\n  0.072  0.7619863  0.5191976\n  0.073  0.7619863  0.5191976\n  0.074  0.7619863  0.5191976\n  0.075  0.7619863  0.5191976\n  0.076  0.7619863  0.5191976\n  0.077  0.7619863  0.5191976\n  0.078  0.7619863  0.5191976\n  0.079  0.7619863  0.5191976\n  0.080  0.7619863  0.5191976\n  0.081  0.7619863  0.5191976\n  0.082  0.7619863  0.5191976\n  0.083  0.7619863  0.5191976\n  0.084  0.7619863  0.5191976\n  0.085  0.7619863  0.5191976\n  0.086  0.7619863  0.5191976\n  0.087  0.7619863  0.5191976\n  0.088  0.7619863  0.5191976\n  0.089  0.7619863  0.5191976\n  0.090  0.7619863  0.5191976\n  0.091  0.7619863  0.5191976\n  0.092  0.7619863  0.5191976\n  0.093  0.7619863  0.5191976\n  0.094  0.7619863  0.5191976\n  0.095  0.7619863  0.5191976\n  0.096  0.7619863  0.5191976\n  0.097  0.7619863  0.5191976\n  0.098  0.7619863  0.5191976\n  0.099  0.7619863  0.5191976\n  0.100  0.7619863  0.5191976\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.019.\n\n# Make predictions on the test set\npredictions_rpart &lt;- predict(rpart_model, newdata = test_data)\n\n# Evaluate the classification tree model on the test set using confusion matrix\nconf_matrix_rpart &lt;- confusionMatrix(predictions_rpart, test_data$HeartDisease)\nprint(conf_matrix_rpart)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  92  27\n         1  31 125\n                                          \n               Accuracy : 0.7891          \n                 95% CI : (0.7361, 0.8358)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.5721          \n                                          \n Mcnemar's Test P-Value : 0.6936          \n                                          \n            Sensitivity : 0.7480          \n            Specificity : 0.8224          \n         Pos Pred Value : 0.7731          \n         Neg Pred Value : 0.8013          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3345          \n   Detection Prevalence : 0.4327          \n      Balanced Accuracy : 0.7852          \n                                          \n       'Positive' Class : 0               \n                                          \n\n### Random Forest Model\n# Set up the grid for tuning the parameter mtry\nnum_predictors &lt;- ncol(train_data) - 1\ntune_grid_rf &lt;- expand.grid(mtry = seq(1, num_predictors, by = 2))\n\n# Random forest model\nrf_model &lt;- train(\n  HeartDisease ~ ., \n  data = train_data,\n  method = \"rf\",\n  trControl = train_control,\n  tuneGrid = tune_grid_rf\n)\n\nrf_model\n\nRandom Forest \n\n643 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 578, 579, 579, 579, 578, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   1    0.8070116  0.6056825\n   3    0.7992702  0.5913028\n   5    0.7972106  0.5881532\n   7    0.7945739  0.5828381\n   9    0.7893571  0.5730156\n  11    0.7873381  0.5686113\n  13    0.7878667  0.5700422\n  15    0.7894294  0.5731779\n  17    0.7863119  0.5670680\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 1.\n\n# Predictions on the test set\npredictions_rf &lt;- predict(rf_model, newdata = test_data)\n\n# Confusion matrix for rf_model predictions\nconf_matrix_rf &lt;- confusionMatrix(predictions_rf, test_data$HeartDisease)\nprint(conf_matrix_rf)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 105  22\n         1  18 130\n                                         \n               Accuracy : 0.8545         \n                 95% CI : (0.8072, 0.894)\n    No Information Rate : 0.5527         \n    P-Value [Acc &gt; NIR] : &lt;2e-16         \n                                         \n                  Kappa : 0.7067         \n                                         \n Mcnemar's Test P-Value : 0.6353         \n                                         \n            Sensitivity : 0.8537         \n            Specificity : 0.8553         \n         Pos Pred Value : 0.8268         \n         Neg Pred Value : 0.8784         \n             Prevalence : 0.4473         \n         Detection Rate : 0.3818         \n   Detection Prevalence : 0.4618         \n      Balanced Accuracy : 0.8545         \n                                         \n       'Positive' Class : 0              \n                                         \n\n### Boosted Tree Model\n# Set up the grid for tuning the parameters\ntune_grid_gbm &lt;- expand.grid(\n  n.trees = c(25, 50, 100, 200),\n  interaction.depth = c(1, 2, 3),\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n# Boosted tree model\ngbm_model &lt;- train(\n  HeartDisease ~ ., \n  data = train_data,\n  method = \"gbm\",\n  trControl = train_control,\n  tuneGrid = tune_grid_gbm,\n  verbose = FALSE\n)\n\ngbm_model\n\nStochastic Gradient Boosting \n\n643 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 578, 579, 578, 579, 579, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.7942808  0.5807800\n  1                   50      0.8046090  0.6033780\n  1                  100      0.7993761  0.5935222\n  1                  200      0.7962506  0.5883810\n  2                   25      0.8056429  0.6035407\n  2                   50      0.8077020  0.6095101\n  2                  100      0.8051213  0.6043725\n  2                  200      0.7983593  0.5911284\n  3                   25      0.8046499  0.6030109\n  3                   50      0.8029824  0.5996513\n  3                  100      0.7999138  0.5934974\n  3                  200      0.7967883  0.5876520\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 2, shrinkage = 0.1 and n.minobsinnode = 10.\n\n# Predictions\npredictions_gbm &lt;- predict(gbm_model, newdata = test_data)\n\n# Confusion matrix for gbm model predictions\nconf_matrix_gbm &lt;- confusionMatrix(predictions_gbm, test_data$HeartDisease)\nprint(conf_matrix_gbm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 108  19\n         1  15 133\n                                          \n               Accuracy : 0.8764          \n                 95% CI : (0.8315, 0.9128)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.7507          \n                                          \n Mcnemar's Test P-Value : 0.6069          \n                                          \n            Sensitivity : 0.8780          \n            Specificity : 0.8750          \n         Pos Pred Value : 0.8504          \n         Neg Pred Value : 0.8986          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3927          \n   Detection Prevalence : 0.4618          \n      Balanced Accuracy : 0.8765          \n                                          \n       'Positive' Class : 0               \n                                          \n\n# Compare models\nmodel_list &lt;- list\n\nTo conclude, we can discuss which model did the best job in terms of accuracy on the test set. In last place, the kNN model returned an accuracy score of .8509 with a 95% confidence interval ranging from (0.8032, 0.8908). In second place was the full predictor logistic regression model with an accuracy score of 0.8618 with a 95% confidence interval of (0.8153, 0.9003). With the highest accuracy score the random forest tree model gave us an accuracy score of 0.8655 with a 95% confidence interval of (0.8193, 0.9035)."
  }
]